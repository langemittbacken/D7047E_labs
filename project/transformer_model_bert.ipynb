{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wd9Sqbs2JTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyGb_taUKsWr",
        "outputId": "722fba23-183a-4b77-ba10-4b03f82b2069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apBfsyqlJB7h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from pytorch_transformers import BertTokenizer\n",
        "from pytorch_transformers import BertModel\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8CASLqJB7m"
      },
      "outputs": [],
      "source": [
        "labels = {'NOT': 0,\n",
        "         'HOF': 1}\n",
        "\n",
        "labels2 = {'NOT': 0,\n",
        "         'OFF': 1}\n",
        "\n",
        "def preprocess(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['tweet'] = data['tweet'].str.lower()\n",
        "    data['tweet'] = data['tweet'].str.replace(r'(@\\S+)', r' ', regex=True)\n",
        "    data['tweet'] = data['tweet'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    data['tweet'] = data['tweet'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    data['tweet'] = data['tweet'].str.replace('[^\\w\\s]', '')                                                       # remove special characters\n",
        "    data['tweet'] = data['tweet'].replace('\\d', '', regex=True)\n",
        "    data['tweet'] = data['tweet'].str.replace(r'[^\\x00-\\x7F]+', r' ', regex=True)\n",
        "    data['tweet'] = data['tweet'].replace('@[a-zA-Z0-9-_]+', '@USER', regex=True)                                     # remove usernames\n",
        "    data['tweet'] = data['tweet'].replace('https://t.co/[a-zA-Z0-9]+', '', regex=True) \n",
        "    \n",
        "    i = 0\n",
        "    for index, row in data.iterrows():\n",
        "        text = row['tweet']\n",
        "        #word_tokens = tokenizer.tokenize(text)\n",
        "        token_id = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "        #filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = df_.append({\n",
        "            \"index\": i,\n",
        "            \"Class\": labels[row['task_1']],\n",
        "            \"Tweet\": text,\n",
        "            'Token_id': token_id\n",
        "        }, ignore_index=True)\n",
        "        i = i + 1\n",
        "    return df_\n",
        "\n",
        "def pre2(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    \n",
        "    i = 0\n",
        "    for index, row in data.iterrows():\n",
        "        text = row['tweet']\n",
        "        token_id = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "        #filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = df_.append({\n",
        "            \"index\": i,\n",
        "            \"Class\": labels2[row['subtask_a']],\n",
        "            \"Tweet\": text,\n",
        "            'Token_id': token_id\n",
        "        }, ignore_index=True)\n",
        "        i = i + 1\n",
        "    return df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKDMvo3dJB7o"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "data1 = pd.read_table('/content/drive/MyDrive/deepNN-labs/data/HASOCData/english_dataset.tsv')\n",
        "#data2 = pd.read_table('/content/drive/MyDrive/deepNN-labs/data/HASOCData/hasoc2019_en_test-2919.tsv')\n",
        "columns = ['index', 'Tweet', 'Token_id','Class']\n",
        "data = preprocess(data1, columns)\n",
        "\n",
        "#dataT = preprocess(data2, columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tex = pd.read_table('/content/drive/MyDrive/deepNN-labs/data/olid_pre/train.txt')\n",
        "\n",
        "data = pre2(tex, columns)"
      ],
      "metadata": {
        "id": "UXsM1yIi7deC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G414CSlpcq-D",
        "outputId": "f4d5d585-0b4b-4331-bc85-5e6aa9e65dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samp = int(0.8*len(data))\n",
        "val_samp = len(data) - train_samp\n",
        "\n",
        "train, val = torch.utils.data.random_split(data, [train_samp, val_samp], generator=torch.Generator())\n",
        "\n",
        "dataloader = DataLoader(train, batch_size=32, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "8JicoIKVga9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        return tweet, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        tweets, labels = zip(*batch)\n",
        "        \n",
        "        # Ensure input_ids and attention_mask are lists of 1D tensors\n",
        "        input_ids = [t['input_ids'].squeeze() for t in tweets]\n",
        "        attention_mask = [t['attention_mask'].squeeze() for t in tweets]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.stack(labels)\n",
        "        }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuugJJFgjrPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = data.iloc[train.indices]\n",
        "val_df = data.iloc[val.indices]\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = TweetDataset(train_df['Token_id'].tolist(), train_df['Class'].tolist())\n",
        "val_dataset = TweetDataset(val_df['Token_id'].tolist(), val_df['Class'].tolist())\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=TweetDataset.collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=TweetDataset.collate_fn)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "# Setup the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# Setup the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # ========================\n",
        "    #   Validation\n",
        "    # ========================\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        with torch.no_grad():        \n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKgGeKzimHe0",
        "outputId": "95b33d1d-b2b5-4541-cb49-6c8727a93c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.79\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.79\n",
            "  Accuracy: 0.75\n",
            "  Accuracy: 0.78\n",
            "  Accuracy: 0.78\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.78\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
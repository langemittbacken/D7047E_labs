{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "data = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=preprocess)\n",
    "test_data = torchvision.datasets.CIFAR10(\"./\", train=False, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9*len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(data, [train_size,test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "SHUFFLE = True\n",
    "LEARNING_RATE = 0.001  \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size= BATCH_SIZE, shuffle= SHUFFLE )\n",
    "test_loader = DataLoader(test_data, batch_size= BATCH_SIZE, shuffle= SHUFFLE)\n",
    "val_loader = DataLoader(val_set, batch_size= BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "model_ft.last_linear = nn.Sequential(\n",
    "    nn.Linear(in_features=100, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=100, out_features=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_accuracy = 0\n",
    "    for epochs in range(num_epochs):\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "         \n",
    "            prediction = model.forward(data)\n",
    "\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(\n",
    "            f'\\rEpoch {epochs+1} [{batch_nr+1}/{len(train_loader)}] - Loss: {loss}',\n",
    "            end=''\n",
    "        )\n",
    "        val_accuracy = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "                prediction = model.forward(data)\n",
    "                _, predicted = torch.max(prediction, 1)\n",
    "                val_accuracy += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                print(val_accuracy/total, \"Val Acc\")\n",
    "            val_accuracy = val_accuracy/total\n",
    "            if(best_accuracy < val_accuracy):\n",
    "                best_accuracy = val_accuracy\n",
    "                print(\"best model here\")\n",
    "                torch.save(model.state_dict(), \"./E2\")\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    test_accuracy = 0\n",
    "    total = 0\n",
    "    predictList = []\n",
    "    testList = []\n",
    "    \n",
    "    for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "        prediction = model.forward(data)\n",
    "        _, predicted = torch.max(prediction, 1)\n",
    "        for i in range(len(prediction)):\n",
    "            predictList.append(predicted[i].item())\n",
    "            testList.append(labels[i].item())\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [3/45] - Loss: 3.3776037693023689"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer_ft \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_ft\u001b[39m.\u001b[39mparameters(), LEARNING_RATE, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trained_model_ft \u001b[39m=\u001b[39m train_model(model_ft, criterion_ft, optimizer_ft, train_loader, val_loader, epochs)\n\u001b[0;32m     11\u001b[0m model_ft\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./E2\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [32], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epochs \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m batch_nr, (data, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 6\u001b[0m         prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(data)\n\u001b[0;32m      8\u001b[0m         loss \u001b[39m=\u001b[39m criterion(prediction, labels)\n\u001b[0;32m     10\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\alexnet.py:51\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[0;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "criterion_ft = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), LEARNING_RATE, weight_decay=1e-2)\n",
    "\n",
    "# Train the model\n",
    "trained_model_ft = train_model(model_ft, criterion_ft, optimizer_ft, train_loader, val_loader, epochs)\n",
    "model_ft.load_state_dict(torch.load(\"./E2\"))\n",
    "# Test the model\n",
    "tested_model = test_model(model_ft, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use resnet18 as the model.\n",
    "model_fe = models.alexnet(pretrained=True)\n",
    "\n",
    "for params in model_fe.parameters():\n",
    "    params.requires_grad_ = False\n",
    "# Do the things required for fine-tuning before training the model\n",
    "model_ft.last_linear = nn.Sequential(\n",
    "    nn.Linear(in_features=100, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=100, out_features=10),\n",
    ")\n",
    "\n",
    "criterion_fe = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "\n",
    "optimizer_fe = optim.Adam(model_fe.parameters(), LEARNING_RATE, weight_decay=1e-2)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Train the model\n",
    "trained_model_fe = train_model(model_fe, criterion_fe, optimizer_fe, train_loader, val_loader, epochs)\n",
    "model_fe.load_state_dict(torch.load(\"./E2\"))\n",
    "# Test the model\n",
    "tested_model = test_model(model_fe, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

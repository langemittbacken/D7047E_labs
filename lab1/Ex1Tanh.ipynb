{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "data = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.CIFAR10(\"./\", train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9*len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(data, [train_size,test_size])\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=10,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=10,\n",
    "                                          shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=10,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [4500/4500] - Loss: 1.5766049623489384  0.3726 Val Acc\n",
      "best model here\n",
      "Epoch 2 [4500/4500] - Loss: 1.2912018299102783  0.434 Val Acc\n",
      "best model here\n",
      "0.4501 Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, msg):\n",
    "    writer = SummaryWriter()\n",
    "    best_accuracy = 0\n",
    "    for epochs in range(num_epochs):\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "         \n",
    "            prediction = model.forward(data)\n",
    "\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(msg, loss, epochs)\n",
    "            print(\n",
    "            f'\\rEpoch {epochs+1} [{batch_nr+1}/{len(train_loader)}] - Loss: {loss}',\n",
    "            end=''\n",
    "        )\n",
    "        val_accuracy = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "                prediction = model.forward(data)\n",
    "                _, predicted = torch.max(prediction, 1)\n",
    "                val_accuracy += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            print(\" \",val_accuracy/total, \"Val Acc\")\n",
    "            val_accuracy = val_accuracy/total\n",
    "            if(best_accuracy < val_accuracy):\n",
    "                best_accuracy = val_accuracy\n",
    "                print(\"best model here\")\n",
    "                torch.save(model.state_dict(), \"./E1\")\n",
    "def test_model(model, test_loader):\n",
    "    test_accuracy = 0\n",
    "    total = 0\n",
    "    predictList = []\n",
    "    testList = []\n",
    "    \n",
    "    for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "        prediction = model.forward(data)\n",
    "        _, predicted = torch.max(prediction, 1)\n",
    "        for i in range(len(prediction)):\n",
    "            predictList.append(predicted[i].item())\n",
    "            testList.append(labels[i].item())\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(test_accuracy/total, \"Test Accuracy\")\n",
    "  \n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Load our network\n",
    "model = Net()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "msg = 'training_loss with tanh and adam'\n",
    "trained_model = train_model(model, criterion, optimizer, trainloader, valloader, 2, msg)\n",
    "\n",
    "# Test the model\n",
    "model.load_state_dict(torch.load(\"./E1\"))\n",
    "tested_model = test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [4500/4500] - Loss: 2.3082318305969247  0.1138 Val Acc\n",
      "best model here\n",
      "Epoch 2 [4500/4500] - Loss: 2.2879505157470703  0.1118 Val Acc\n",
      "0.109 Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), LEARNING_RATE)\n",
    "msg = 'training_loss with tanh and SGD'\n",
    "trained_model = train_model(model, criterion, optimizer, trainloader, valloader, 2, msg)\n",
    "model.load_state_dict(torch.load(\"./E1\"))\n",
    "tested_model = test_model(model, testloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Adam as the optimizer and Tanh as the activation function gives roughly 44% accuracy with a learning rate of 0.0001\n",
    "\n",
    "\n",
    "Using SGD as the optimizer and Tanh as the activation function gives roughly 10% accuracy with a learning rate of 0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TanhADAM](tanhADAM.png)\n",
    "![TanhSGD](tanhSGD.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

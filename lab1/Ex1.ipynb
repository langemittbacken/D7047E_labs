{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "data = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = torchvision.datasets.CIFAR10(\"./\", train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9*len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(data, [train_size,test_size])\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=10,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=10,\n",
    "                                          shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=10,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [4500/4500] - Loss: 1.7928653955459595  0.4104 Val Acc\n",
      "best model here\n",
      "Epoch 2 [4500/4500] - Loss: 1.8299224376678467  0.4288 Val Acc\n",
      "best model here\n",
      "0.4228 Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Implement a train model function so you can re_use it in task 3 and 4. \n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, msg):\n",
    "    best_accuracy = 0\n",
    "    writer = SummaryWriter()\n",
    "    for epochs in range(num_epochs):\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "         \n",
    "            prediction = model.forward(data)\n",
    "\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(msg, loss, epochs)\n",
    "            print(\n",
    "            f'\\rEpoch {epochs+1} [{batch_nr+1}/{len(train_loader)}] - Loss: {loss}',\n",
    "            end=''\n",
    "        )\n",
    "        val_accuracy = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "                prediction = model.forward(data)\n",
    "                _, predicted = torch.max(prediction, 1)\n",
    "                val_accuracy += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            print(\" \",val_accuracy/total, \"Val Acc\")\n",
    "            val_accuracy = val_accuracy/total\n",
    "            if(best_accuracy < val_accuracy):\n",
    "                best_accuracy = val_accuracy\n",
    "                print(\"best model here\")\n",
    "                torch.save(model.state_dict(), \"./E1\")\n",
    "def test_model(model, test_loader):\n",
    "    test_accuracy = 0\n",
    "    total = 0\n",
    "    predictList = []\n",
    "    testList = []\n",
    "    \n",
    "    for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "        prediction = model.forward(data)\n",
    "        _, predicted = torch.max(prediction, 1)\n",
    "        for i in range(len(prediction)):\n",
    "            predictList.append(predicted[i].item())\n",
    "            testList.append(labels[i].item())\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(test_accuracy/total, \"Test Accuracy\")\n",
    "  \n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Load our network\n",
    "model = Net()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "msg = \"trainloss leaky relu and adam\"\n",
    "trained_model = train_model(model, criterion, optimizer, trainloader, valloader, 2, msg)\n",
    "\n",
    "# Test the model\n",
    "model.load_state_dict(torch.load(\"./E1\"))\n",
    "tested_model = test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [4500/4500] - Loss: 2.3100845813751224  0.0922 Val Acc\n",
      "best model here\n",
      "Epoch 2 [4500/4500] - Loss: 2.2878556251525886  0.0946 Val Acc\n",
      "best model here\n",
      "0.1032 Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), LEARNING_RATE)\n",
    "msg = \"trainloss leaky and SGD\"\n",
    "trained_model = train_model(model, criterion, optimizer, trainloader, valloader, 2, msg)\n",
    "model.load_state_dict(torch.load(\"./E1\"))\n",
    "tested_model = test_model(model, testloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Adam as the optimizer gives roughly 44% accuracy with a learning rate of 0.0001\n",
    "\n",
    "\n",
    "Using SGD as the optimizer gives roughly 10% accuracy with a learning rate of 0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![leakyADAM](leakyADAM.png)\n",
    "![leakySGD](leakySGD.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a2bbd04d22b0b729b0b7f054f479d5edfef8abe659c6664693311608c6e53e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
